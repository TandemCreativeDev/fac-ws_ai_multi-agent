def run_pattern_comparison():
    """
    Compare different LangGraph patterns for code review
    Run this to understand when to use each pattern
    """

    patterns = {
        "Sequential": {
            "file": "01_sequential_workflow.py",
            "description": "Linear pipeline: coder ‚Üí reviewer ‚Üí refactorer",
            "best_for": "Simple, predictable workflows",
            "complexity": "Low",
            "execution_time": "Fast",
            "use_cases": ["Basic automation", "Simple validation", "Learning LangGraph"]
        },

        "Conditional Routing": {
            "file": "02_conditional_routing.py",
            "description": "Quality-based routing with improvement loops",
            "best_for": "Quality-dependent workflows",
            "complexity": "Medium",
            "execution_time": "Variable",
            "use_cases": ["Content moderation", "Quality assurance", "Iterative improvement"]
        },

        "Parallel Processing": {
            "file": "03_parallel_processing.py",
            "description": "Concurrent analysis by multiple specialists",
            "best_for": "Independent, parallelisable tasks",
            "complexity": "Medium",
            "execution_time": "Fast (parallel)",
            "use_cases": ["Document processing", "Multi-aspect analysis", "Performance optimisation"]
        },

        "Supervisor Agents": {
            "file": "04_supervisor_agents.py",
            "description": "Intelligent coordination of specialist agents",
            "best_for": "Complex tasks requiring expertise",
            "complexity": "High",
            "execution_time": "Efficient",
            "use_cases": ["Complex analysis", "Domain expertise", "Dynamic workflows"]
        },

        "Evaluator-Optimiser": {
            "file": "05_evaluator_optimiser.py",
            "description": "Continuous improvement through feedback loops",
            "best_for": "Iteratively improvable outputs",
            "complexity": "High",
            "execution_time": "Slow (iterative)",
            "use_cases": ["Content generation", "Optimisation tasks", "Quality refinement"]
        },

        "Production Ready": {
            "file": "06_production_ready.py",
            "description": "Enterprise features: error handling, persistence, monitoring",
            "best_for": "Real-world deployment",
            "complexity": "Very High",
            "execution_time": "Robust",
            "use_cases": ["Production systems", "Enterprise deployment", "Mission-critical tasks"]
        }
    }

    print("=== LANGGRAPH PATTERN COMPARISON ===\n")

    for pattern_name, details in patterns.items():
        print(f"üìã {pattern_name}")
        print(f"   File: {details['file']}")
        print(f"   Description: {details['description']}")
        print(f"   Best for: {details['best_for']}")
        print(f"   Complexity: {details['complexity']}")
        print(f"   Execution: {details['execution_time']}")
        print(f"   Use cases: {', '.join(details['use_cases'])}")
        print()

    print("=== DECISION MATRIX ===\n")

    scenarios = [
        {
            "scenario": "Simple blog post review",
            "recommended": "Sequential",
            "reason": "Predictable workflow, no complex logic needed"
        },
        {
            "scenario": "Code security analysis",
            "recommended": "Parallel Processing",
            "reason": "Multiple independent analyses can run concurrently"
        },
        {
            "scenario": "Complex enterprise system review",
            "recommended": "Supervisor Agents",
            "reason": "Requires domain expertise and intelligent coordination"
        },
        {
            "scenario": "Creative content generation",
            "recommended": "Evaluator-Optimiser",
            "reason": "Benefits from iterative feedback and improvement"
        },
        {
            "scenario": "Mission-critical financial system",
            "recommended": "Production Ready",
            "reason": "Requires robust error handling and monitoring"
        }
    ]

    for scenario in scenarios:
        print(f"üéØ Scenario: {scenario['scenario']}")
        print(f"   Recommended: {scenario['recommended']}")
        print(f"   Reason: {scenario['reason']}")
        print()

    print("=== PATTERN EVOLUTION ===\n")
    print("Start Simple ‚Üí Add Intelligence ‚Üí Scale Performance ‚Üí Gain Expertise ‚Üí Ensure Quality ‚Üí Deploy Safely")
    print()
    print("Sequential ‚Üí Conditional ‚Üí Parallel ‚Üí Supervisor ‚Üí Evaluator ‚Üí Production")
    print()

    print("=== IMPLEMENTATION GUIDE ===\n")
    print("1. Always start with Sequential for prototyping")
    print("2. Add Conditional routing when you need quality gates")
    print("3. Use Parallel when you have independent tasks")
    print("4. Employ Supervisor for complex domain-specific tasks")
    print("5. Add Evaluator-Optimiser for quality-critical outputs")
    print("6. Implement Production patterns for real deployment")
    print()

    print("=== ANTI-PATTERNS ===\n")
    print("‚ùå Using Supervisor for simple linear tasks")
    print("‚ùå Parallel processing for sequential dependencies")
    print("‚ùå Evaluator-Optimiser for time-critical tasks")
    print("‚ùå Sequential for complex multi-domain problems")
    print("‚ùå Skipping Production patterns for real deployment")


def benchmark_patterns():
    """
    Theoretical performance comparison
    (Run actual files for real benchmarks)
    """

    print("\n=== THEORETICAL PERFORMANCE ===\n")

    performance_data = {
        "Sequential": {"latency": "1x", "complexity": "O(n)", "scalability": "Linear"},
        "Conditional": {"latency": "1-3x", "complexity": "O(n*k)", "scalability": "Variable"},
        "Parallel": {"latency": "0.3x", "complexity": "O(n)", "scalability": "Horizontal"},
        "Supervisor": {"latency": "1.2x", "complexity": "O(n*log(n))", "scalability": "Intelligent"},
        "Evaluator-Optimiser": {"latency": "3-10x", "complexity": "O(n¬≤)", "scalability": "Iterative"},
        "Production": {"latency": "1.5x", "complexity": "O(n)", "scalability": "Robust"}
    }

    print("Pattern                 | Latency | Complexity  | Scalability")
    print("-" * 60)

    for pattern, metrics in performance_data.items():
        latency = metrics["latency"].ljust(7)
        complexity = metrics["complexity"].ljust(11)
        scalability = metrics["scalability"]
        print(f"{pattern.ljust(22)} | {latency} | {complexity} | {scalability}")


if __name__ == "__main__":
    run_pattern_comparison()
    benchmark_patterns()

    print("\n=== NEXT STEPS ===")
    print("1. Run each pattern file to see them in action")
    print("2. Experiment with modifying prompts and logic")
    print("3. Combine patterns for your specific use case")
    print("4. Deploy using LangGraph Platform for production")
